{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40204ac0",
   "metadata": {},
   "source": [
    "# 04. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36598c38",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The goal of this notebook is to prepare the dataset for machine learning by creating, transforming, and encoding features that improve the model’s ability to predict CEFR levels. Specifically, we aim to:\n",
    "\n",
    "- Encode the categorical target variable (CEFR levels) into numerical labels suitable for classification.  \n",
    "- Ensure feature scaling/normalization so that all skill scores contribute fairly to the model.  \n",
    "- Explore potential derived features (e.g., average score, skill differences) that may enhance predictive performance.  \n",
    "- Split the dataset into training and test sets with stratification to preserve class distribution.  \n",
    "- Generate a final feature matrix (`X`) and target vector (`y`) ready for model training.  \n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- Cleaned dataset: `data/clean/cleaned_lang_proficiency_results.csv`  \n",
    "- Columns: `speaking_score`, `reading_score`, `listening_score`, `writing_score`, `overall_cefr`  \n",
    "\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Encoded target labels for CEFR levels  \n",
    "- Scaled/normalized feature set  \n",
    "- Optional engineered features (e.g., mean score, modality balance)  \n",
    "- Train/test splits saved for modeling  \n",
    "- Final processed dataset in a format ready for the ML notebook  \n",
    "\n",
    "\n",
    "## Additional Information\n",
    "\n",
    "Feature engineering bridges the gap between raw data and machine learning readiness. Since the business requirement is **automatic learner placement and personalized recommendations**, ensuring that CEFR levels can be predicted accurately depends on well-prepared features. In this step, we transform the raw language skill scores into an optimized input space for classification models, laying the foundation for robust and interpretable predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43581c5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b591f7",
   "metadata": {},
   "source": [
    "# Project Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2af5b7",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "\n",
    "We need to change the working directory from its current folder to the folder the code of this project is currently located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea55cf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\husse\\\\OneDrive\\\\Projects\\\\lang-level-pred\\\\jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304e7bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\husse\\OneDrive\\Projects\\lang-level-pred\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# swtich to project root directory\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7fe9ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c08e6d",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "This code block imports fundamental Python libraries for data analysis and visualization and checks their versions\n",
    "\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computations\n",
    "- matplotlib: For creating visualizations and plots\n",
    "- seaborn: creating attractive and informative statistical graphics from datasets\n",
    "\n",
    "The version checks help ensure:\n",
    "- Code compatibility across different environments\n",
    "- Reproducibility of analysis\n",
    "- Easy debugging of version-specific issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752ed86",
   "metadata": {},
   "source": [
    "### List Files and Folders\n",
    "- This code shows what files and folders are in our data/clean folder and what folder we are currently in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418fe02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Files/folders available in data\\clean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cleaned_lang_proficiency_results.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = Path(\"data/clean\")\n",
    "print(f\"[INFO] Files/folders available in {dataset_dir}:\")\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cb192",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "This code loads the dataset from the data/clean folder that is then displayed in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "255ae484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaking_score</th>\n",
       "      <th>reading_score</th>\n",
       "      <th>listening_score</th>\n",
       "      <th>writing_score</th>\n",
       "      <th>overall_cefr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>79</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speaking_score  reading_score  listening_score  writing_score overall_cefr\n",
       "0              24             38               30             34           A1\n",
       "1              93             91               90             89           C1\n",
       "2              62             64               64             55           B1\n",
       "3              63             59               54             54           B1\n",
       "4              79             74               85             79           B2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = Path(\"data/clean/cleaned_lang_proficiency_results.csv\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c14f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f7b2a",
   "metadata": {},
   "source": [
    "## 1. Target Variable Encoding\n",
    "\n",
    "The target variable `overall_cefr` is categorical (A1–C2).  \n",
    "To use it in machine learning classification models, we need to encode it into numeric labels.  \n",
    "\n",
    "We will apply the following mapping:\n",
    "\n",
    "- A1 → 0  \n",
    "- A2 → 1  \n",
    "- B1 → 2  \n",
    "- B2 → 3  \n",
    "- C1 → 4  \n",
    "- C2 → 5  \n",
    "\n",
    "This preserves the natural order of proficiency levels while making the target compatible with scikit-learn classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5beb120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1004 entries, 0 to 1003\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   speaking_score   1004 non-null   int64 \n",
      " 1   reading_score    1004 non-null   int64 \n",
      " 2   listening_score  1004 non-null   int64 \n",
      " 3   writing_score    1004 non-null   int64 \n",
      " 4   overall_cefr     1004 non-null   object\n",
      " 5   cefr_encoded     1004 non-null   int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 47.2+ KB\n",
      "✅ Target variable encoded successfully!\n",
      "  overall_cefr  cefr_encoded\n",
      "0           A1             0\n",
      "1           C1             4\n",
      "2           B1             2\n",
      "3           B1             2\n",
      "4           B2             3\n",
      "5           B1             2\n",
      "6           A2             1\n",
      "7           B2             3\n",
      "8           A2             1\n",
      "9           B1             2\n",
      "\n",
      "Encoded CEFR distribution:\n",
      "cefr_encoded\n",
      "0    208\n",
      "1    219\n",
      "2    208\n",
      "3    192\n",
      "4     94\n",
      "5     83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define mapping for CEFR levels\n",
    "cefr_mapping = {\"A1\": 0, \"A2\": 1, \"B1\": 2, \"B2\": 3, \"C1\": 4, \"C2\": 5}\n",
    "\n",
    "# Apply mapping to target column\n",
    "df[\"cefr_encoded\"] = df[\"overall_cefr\"].map(cefr_mapping)\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Verify encoding\n",
    "print(\"✅ Target variable encoded successfully!\")\n",
    "print(df[[\"overall_cefr\", \"cefr_encoded\"]].head(10))\n",
    "\n",
    "# Check value counts to confirm distribution\n",
    "print(\"\\nEncoded CEFR distribution:\")\n",
    "print(df[\"cefr_encoded\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3a180",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "In this step, we create additional features from the four base skill scores (*speaking, reading, listening, writing*).  \n",
    "These engineered features provide richer insights into learner profiles and enhance the model’s ability to predict CEFR levels.\n",
    "\n",
    "The following **7 engineered features** are created:\n",
    "\n",
    "1. **strongest_skill** → identifies the learner’s best-performing skill.  \n",
    "2. **weakest_skill** → identifies the learner’s lowest-performing skill (main bottleneck).  \n",
    "3. **second_weakest_skill** → captures the learner’s secondary weakness for targeted recommendations.  \n",
    "4. **skill_std** → standard deviation across the four skills, measuring balance vs imbalance.  \n",
    "5. **strength_weakness_gap** → difference between strongest and weakest skill scores.  \n",
    "6. **productive_receptive_ratio** → ratio of productive (speaking + writing) to receptive (reading + listening) skills.  \n",
    "7. **learning_profile** → categorical label:  \n",
    "   - *Balanced* → skills are evenly developed (low variance).  \n",
    "   - *Uneven Development* → large discrepancies between strengths and weaknesses.  \n",
    "8. **speaking_minus_avg** → difference between speaking score and learner’s average.  \n",
    "9. **reading_minus_avg** → difference between reading score and learner’s average.  \n",
    "10. **listening_minus_avg** → difference between listening score and learner’s average.  \n",
    "11. **writing_minus_avg** → difference between writing score and learner’s average.  \n",
    "12. **speaking_to_reading** → ratio of speaking score to reading score.  \n",
    "13. **writing_to_listening** → ratio of writing score to listening score.\n",
    "\n",
    "**Important: Preventing Data Leakage**  \n",
    "The raw scores (*speaking_score, reading_score, listening_score, writing_score*) are highly correlated with CEFR and would cause the model to “cheat” by memorizing direct score-to-level mappings.  \n",
    "To ensure fairness and generalization, we **drop the raw scores after feature engineering**, keeping only the derived features listed above.  \n",
    "\n",
    "This ensures the model learns from **relative skill patterns and learner profiles**, not from absolute exam results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53f1a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gap threshold for Uneven Development: 11.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_cefr</th>\n",
       "      <th>cefr_encoded</th>\n",
       "      <th>strongest_skill</th>\n",
       "      <th>weakest_skill</th>\n",
       "      <th>second_weakest_skill</th>\n",
       "      <th>skill_std</th>\n",
       "      <th>strength_weakness_gap</th>\n",
       "      <th>productive_receptive_ratio</th>\n",
       "      <th>learning_profile</th>\n",
       "      <th>speaking_minus_avg</th>\n",
       "      <th>reading_minus_avg</th>\n",
       "      <th>listening_minus_avg</th>\n",
       "      <th>writing_minus_avg</th>\n",
       "      <th>speaking_to_reading</th>\n",
       "      <th>writing_to_listening</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>0</td>\n",
       "      <td>reading</td>\n",
       "      <td>speaking</td>\n",
       "      <td>listening</td>\n",
       "      <td>5.972158</td>\n",
       "      <td>14</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>Uneven Development</td>\n",
       "      <td>-7.50</td>\n",
       "      <td>6.50</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1</td>\n",
       "      <td>4</td>\n",
       "      <td>speaking</td>\n",
       "      <td>writing</td>\n",
       "      <td>listening</td>\n",
       "      <td>1.707825</td>\n",
       "      <td>4</td>\n",
       "      <td>1.005525</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>1.021978</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>2</td>\n",
       "      <td>reading</td>\n",
       "      <td>writing</td>\n",
       "      <td>speaking</td>\n",
       "      <td>4.272002</td>\n",
       "      <td>9</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-6.25</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>2</td>\n",
       "      <td>speaking</td>\n",
       "      <td>listening</td>\n",
       "      <td>listening</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>9</td>\n",
       "      <td>1.035398</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>1.067797</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "      <td>listening</td>\n",
       "      <td>reading</td>\n",
       "      <td>speaking</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.993711</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>5.75</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.067568</td>\n",
       "      <td>0.929412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  overall_cefr  cefr_encoded strongest_skill weakest_skill  \\\n",
       "0           A1             0         reading      speaking   \n",
       "1           C1             4        speaking       writing   \n",
       "2           B1             2         reading       writing   \n",
       "3           B1             2        speaking     listening   \n",
       "4           B2             3       listening       reading   \n",
       "\n",
       "  second_weakest_skill  skill_std  strength_weakness_gap  \\\n",
       "0            listening   5.972158                     14   \n",
       "1            listening   1.707825                      4   \n",
       "2             speaking   4.272002                      9   \n",
       "3            listening   4.358899                      9   \n",
       "4             speaking   4.500000                     11   \n",
       "\n",
       "   productive_receptive_ratio    learning_profile  speaking_minus_avg  \\\n",
       "0                    0.852941  Uneven Development               -7.50   \n",
       "1                    1.005525            Balanced                2.25   \n",
       "2                    0.914062            Balanced                0.75   \n",
       "3                    1.035398            Balanced                5.50   \n",
       "4                    0.993711            Balanced               -0.25   \n",
       "\n",
       "   reading_minus_avg  listening_minus_avg  writing_minus_avg  \\\n",
       "0               6.50                -1.50               2.50   \n",
       "1               0.25                -0.75              -1.75   \n",
       "2               2.75                 2.75              -6.25   \n",
       "3               1.50                -3.50              -3.50   \n",
       "4              -5.25                 5.75              -0.25   \n",
       "\n",
       "   speaking_to_reading  writing_to_listening  \n",
       "0             0.631579              1.133333  \n",
       "1             1.021978              0.988889  \n",
       "2             0.968750              0.859375  \n",
       "3             1.067797              1.000000  \n",
       "4             1.067568              0.929412  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List of core skill columns\n",
    "skill_cols = [\"speaking_score\", \"reading_score\", \"listening_score\", \"writing_score\"]\n",
    "\n",
    "# Strongest and weakest skills\n",
    "df[\"strongest_skill\"] = df[skill_cols].idxmax(axis=1).str.replace(\"_score\", \"\")\n",
    "df[\"weakest_skill\"] = df[skill_cols].idxmin(axis=1).str.replace(\"_score\", \"\")\n",
    "\n",
    "# Second weakest skill\n",
    "df[\"second_weakest_skill\"] = df[skill_cols].apply(\n",
    "    lambda row: row.sort_values().index[1].replace(\"_score\", \"\"), axis=1\n",
    ")\n",
    "\n",
    "# Skill standard deviation (imbalance indicator)\n",
    "df[\"skill_std\"] = df[skill_cols].std(axis=1)\n",
    "\n",
    "# Gap between strongest and weakest\n",
    "df[\"strength_weakness_gap\"] = df[skill_cols].max(axis=1) - df[skill_cols].min(axis=1)\n",
    "\n",
    "# Productive (speaking+writing) vs receptive (reading+listening) ratio\n",
    "df[\"productive_receptive_ratio\"] = (\n",
    "    (df[\"speaking_score\"] + df[\"writing_score\"]) /\n",
    "    (df[\"reading_score\"] + df[\"listening_score\"] + 1e-6)  # avoid division by zero\n",
    ")\n",
    "\n",
    "# Learning profile classification\n",
    "gap_threshold = df[\"strength_weakness_gap\"].quantile(0.75)\n",
    "print(f\"Gap threshold for Uneven Development: {gap_threshold:.2f}\")\n",
    "\n",
    "df[\"learning_profile\"] = np.where(\n",
    "    df[\"strength_weakness_gap\"] > gap_threshold,\n",
    "    \"Uneven Development\",\n",
    "    \"Balanced\"\n",
    ")\n",
    "\n",
    "# 🔧 New engineered features\n",
    "\n",
    "# Relative strength of each skill compared to learner’s average\n",
    "df[\"speaking_minus_avg\"] = df[\"speaking_score\"] - df[skill_cols].mean(axis=1)\n",
    "df[\"reading_minus_avg\"] = df[\"reading_score\"] - df[skill_cols].mean(axis=1)\n",
    "df[\"listening_minus_avg\"] = df[\"listening_score\"] - df[skill_cols].mean(axis=1)\n",
    "df[\"writing_minus_avg\"] = df[\"writing_score\"] - df[skill_cols].mean(axis=1)\n",
    "\n",
    "# Extra ratios (pairwise imbalances)\n",
    "df[\"speaking_to_reading\"] = df[\"speaking_score\"] / (df[\"reading_score\"] + 1e-6)\n",
    "df[\"writing_to_listening\"] = df[\"writing_score\"] / (df[\"listening_score\"] + 1e-6)\n",
    "\n",
    "\n",
    "# ✅ Drop raw scores AFTER all feature engineering\n",
    "df = df.drop(columns=skill_cols)\n",
    "\n",
    "# Preview final engineered dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8665c",
   "metadata": {},
   "source": [
    "## 2. Scaling / Normalization\n",
    "\n",
    "Machine learning models are sensitive to the scale of input features, especially distance-based methods (e.g., KNN, SVM) and gradient-based optimizers (e.g., Logistic Regression, Neural Networks).  \n",
    "Since the skill scores (0–100) and engineered features (e.g., ratios, gaps) exist on different scales, we standardize all numeric features to have:\n",
    "\n",
    "- Mean = 0  \n",
    "- Standard Deviation = 1  \n",
    "\n",
    "This ensures that each feature contributes fairly to the model and avoids bias toward high-magnitude features.  \n",
    "We use **StandardScaler** from scikit-learn for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6e448d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strength_weakness_gap</th>\n",
       "      <th>skill_std</th>\n",
       "      <th>productive_receptive_ratio</th>\n",
       "      <th>speaking_minus_avg</th>\n",
       "      <th>reading_minus_avg</th>\n",
       "      <th>listening_minus_avg</th>\n",
       "      <th>writing_minus_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.427690</td>\n",
       "      <td>1.209576</td>\n",
       "      <td>-1.584816</td>\n",
       "      <td>-2.069500</td>\n",
       "      <td>1.751171</td>\n",
       "      <td>-0.421848</td>\n",
       "      <td>0.720782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.225282</td>\n",
       "      <td>-1.273163</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>0.609820</td>\n",
       "      <td>0.058795</td>\n",
       "      <td>-0.221437</td>\n",
       "      <td>-0.438501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.101204</td>\n",
       "      <td>0.219728</td>\n",
       "      <td>-0.943540</td>\n",
       "      <td>0.197617</td>\n",
       "      <td>0.735745</td>\n",
       "      <td>0.713814</td>\n",
       "      <td>-1.665976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101204</td>\n",
       "      <td>0.270320</td>\n",
       "      <td>0.329497</td>\n",
       "      <td>1.502926</td>\n",
       "      <td>0.397270</td>\n",
       "      <td>-0.956277</td>\n",
       "      <td>-0.915852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631798</td>\n",
       "      <td>0.352470</td>\n",
       "      <td>-0.107883</td>\n",
       "      <td>-0.077185</td>\n",
       "      <td>-1.430496</td>\n",
       "      <td>1.515458</td>\n",
       "      <td>-0.029342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   strength_weakness_gap  skill_std  productive_receptive_ratio  \\\n",
       "0               1.427690   1.209576                   -1.584816   \n",
       "1              -1.225282  -1.273163                    0.016070   \n",
       "2               0.101204   0.219728                   -0.943540   \n",
       "3               0.101204   0.270320                    0.329497   \n",
       "4               0.631798   0.352470                   -0.107883   \n",
       "\n",
       "   speaking_minus_avg  reading_minus_avg  listening_minus_avg  \\\n",
       "0           -2.069500           1.751171            -0.421848   \n",
       "1            0.609820           0.058795            -0.221437   \n",
       "2            0.197617           0.735745             0.713814   \n",
       "3            1.502926           0.397270            -0.956277   \n",
       "4           -0.077185          -1.430496             1.515458   \n",
       "\n",
       "   writing_minus_avg  \n",
       "0           0.720782  \n",
       "1          -0.438501  \n",
       "2          -1.665976  \n",
       "3          -0.915852  \n",
       "4          -0.029342  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 6: Scaling / Normalization ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select feature columns (exclude target)\n",
    "feature_cols = [\n",
    "    \"strength_weakness_gap\", \"skill_std\",\n",
    "    \"productive_receptive_ratio\",\n",
    "    \"speaking_minus_avg\", \"reading_minus_avg\", \"listening_minus_avg\", \"writing_minus_avg\",\n",
    "]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "X_scaled = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "# ✅ Replace original columns with scaled ones\n",
    "df[feature_cols] = X_scaled_df\n",
    "\n",
    "# Preview scaled features\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d37e2",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split (with Categorical Encoding)\n",
    "\n",
    "In this step, we prepare the dataset for model training by ensuring the data is correctly split and all features are numerical.  \n",
    "\n",
    "**Process:**  \n",
    "1. **Split the dataset** into training and testing sets using stratified sampling to preserve the distribution of CEFR levels.  \n",
    "2. **Identify categorical engineered features**:  \n",
    "   - `strongest_skill`  \n",
    "   - `weakest_skill`  \n",
    "   - `second_weakest_skill`  \n",
    "   - `learning_profile`  \n",
    "3. **Encode categorical features** using **One-Hot Encoding (OHE)** to convert them into numerical dummy variables.  \n",
    "4. **Retain numeric features** (already scaled in Step 2) without further modification.  \n",
    "5. Combine the numeric and encoded categorical features to form the final training and test sets (`X_train_final`, `X_test_final`).  \n",
    "\n",
    "This ensures that the input dataset is fully numeric, consistent, and ready for use in the **Modeling & Evaluation notebook**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (before encoding): (803, 13)\n",
      "Test set shape (before encoding): (201, 13)\n",
      "Training set shape (after encoding): (803, 19)\n",
      "Test set shape (after encoding): (201, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_std</th>\n",
       "      <th>strength_weakness_gap</th>\n",
       "      <th>productive_receptive_ratio</th>\n",
       "      <th>speaking_minus_avg</th>\n",
       "      <th>reading_minus_avg</th>\n",
       "      <th>listening_minus_avg</th>\n",
       "      <th>writing_minus_avg</th>\n",
       "      <th>speaking_to_reading</th>\n",
       "      <th>writing_to_listening</th>\n",
       "      <th>strongest_skill_reading</th>\n",
       "      <th>strongest_skill_speaking</th>\n",
       "      <th>strongest_skill_writing</th>\n",
       "      <th>weakest_skill_reading</th>\n",
       "      <th>weakest_skill_speaking</th>\n",
       "      <th>weakest_skill_writing</th>\n",
       "      <th>second_weakest_skill_reading</th>\n",
       "      <th>second_weakest_skill_speaking</th>\n",
       "      <th>second_weakest_skill_writing</th>\n",
       "      <th>learning_profile_Uneven Development</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1.623618</td>\n",
       "      <td>1.692987</td>\n",
       "      <td>-1.607845</td>\n",
       "      <td>0.266317</td>\n",
       "      <td>2.428122</td>\n",
       "      <td>-1.089884</td>\n",
       "      <td>-1.597783</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0.903656</td>\n",
       "      <td>1.162393</td>\n",
       "      <td>-0.041896</td>\n",
       "      <td>-1.794698</td>\n",
       "      <td>0.397270</td>\n",
       "      <td>-0.421848</td>\n",
       "      <td>1.811872</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>1.258064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>-0.343831</td>\n",
       "      <td>-0.429390</td>\n",
       "      <td>0.027129</td>\n",
       "      <td>1.022023</td>\n",
       "      <td>-0.618156</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>-0.847659</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0.519644</td>\n",
       "      <td>0.631798</td>\n",
       "      <td>-0.624777</td>\n",
       "      <td>0.609820</td>\n",
       "      <td>0.058795</td>\n",
       "      <td>1.114636</td>\n",
       "      <td>-1.802363</td>\n",
       "      <td>1.025316</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>-0.115134</td>\n",
       "      <td>-0.164093</td>\n",
       "      <td>-0.504772</td>\n",
       "      <td>-0.420688</td>\n",
       "      <td>1.480391</td>\n",
       "      <td>-0.689062</td>\n",
       "      <td>-0.370308</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>1.015625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     skill_std  strength_weakness_gap  productive_receptive_ratio  \\\n",
       "575   1.623618               1.692987                   -1.607845   \n",
       "576   0.903656               1.162393                   -0.041896   \n",
       "914  -0.343831              -0.429390                    0.027129   \n",
       "665   0.519644               0.631798                   -0.624777   \n",
       "632  -0.115134              -0.164093                   -0.504772   \n",
       "\n",
       "     speaking_minus_avg  reading_minus_avg  listening_minus_avg  \\\n",
       "575            0.266317           2.428122            -1.089884   \n",
       "576           -1.794698           0.397270            -0.421848   \n",
       "914            1.022023          -0.618156             0.446600   \n",
       "665            0.609820           0.058795             1.114636   \n",
       "632           -0.420688           1.480391            -0.689062   \n",
       "\n",
       "     writing_minus_avg  speaking_to_reading  writing_to_listening  \\\n",
       "575          -1.597783             0.800000              0.925926   \n",
       "576           1.811872             0.764706              1.258064   \n",
       "914          -0.847659             1.081081              0.935897   \n",
       "665          -1.802363             1.025316              0.867470   \n",
       "632          -0.370308             0.902778              1.015625   \n",
       "\n",
       "     strongest_skill_reading  strongest_skill_speaking  \\\n",
       "575                      1.0                       0.0   \n",
       "576                      0.0                       0.0   \n",
       "914                      0.0                       1.0   \n",
       "665                      0.0                       0.0   \n",
       "632                      1.0                       0.0   \n",
       "\n",
       "     strongest_skill_writing  weakest_skill_reading  weakest_skill_speaking  \\\n",
       "575                      0.0                    0.0                     0.0   \n",
       "576                      1.0                    0.0                     1.0   \n",
       "914                      0.0                    0.0                     0.0   \n",
       "665                      0.0                    0.0                     0.0   \n",
       "632                      0.0                    0.0                     0.0   \n",
       "\n",
       "     weakest_skill_writing  second_weakest_skill_reading  \\\n",
       "575                    1.0                           0.0   \n",
       "576                    0.0                           0.0   \n",
       "914                    1.0                           1.0   \n",
       "665                    1.0                           1.0   \n",
       "632                    0.0                           0.0   \n",
       "\n",
       "     second_weakest_skill_speaking  second_weakest_skill_writing  \\\n",
       "575                            0.0                           0.0   \n",
       "576                            0.0                           0.0   \n",
       "914                            0.0                           0.0   \n",
       "665                            0.0                           0.0   \n",
       "632                            1.0                           0.0   \n",
       "\n",
       "     learning_profile_Uneven Development  \n",
       "575                                  1.0  \n",
       "576                                  1.0  \n",
       "914                                  0.0  \n",
       "665                                  0.0  \n",
       "632                                  0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Test Split with Categorical Encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define target\n",
    "y = df[\"cefr_encoded\"]\n",
    "\n",
    "# Features (drop raw CEFR columns)\n",
    "X = df.drop(columns=[\"overall_cefr\", \"cefr_encoded\"])\n",
    "\n",
    "# Train/Test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape (before encoding):\", X_train.shape)\n",
    "print(\"Test set shape (before encoding):\", X_test.shape)\n",
    "\n",
    "# ---- Handle Categorical Features ---- #\n",
    "categorical_features = [\"strongest_skill\", \"weakest_skill\", \"second_weakest_skill\", \"learning_profile\"]\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(drop=\"first\", sparse_output=False)  # drop=\"first\" avoids dummy trap\n",
    "\n",
    "# Fit on training set only (to avoid data leakage)\n",
    "X_train_cat = ohe.fit_transform(X_train[categorical_features])\n",
    "X_test_cat = ohe.transform(X_test[categorical_features])\n",
    "\n",
    "# Get feature names after encoding\n",
    "ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "X_train_cat = pd.DataFrame(X_train_cat, columns=ohe_feature_names, index=X_train.index)\n",
    "X_test_cat = pd.DataFrame(X_test_cat, columns=ohe_feature_names, index=X_test.index)\n",
    "\n",
    "# Drop original categorical columns and join encoded ones\n",
    "X_train_final = pd.concat([X_train.drop(columns=categorical_features), X_train_cat], axis=1)\n",
    "X_test_final = pd.concat([X_test.drop(columns=categorical_features), X_test_cat], axis=1)\n",
    "\n",
    "print(\"Training set shape (after encoding):\", X_train_final.shape)\n",
    "print(\"Test set shape (after encoding):\", X_test_final.shape)\n",
    "\n",
    "# Quick check on final dataset\n",
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a213938",
   "metadata": {},
   "source": [
    "## 4. Final Feature Set Overview  \n",
    "\n",
    "In this step, we will validate the final dataset before moving to the **Modelling & Evaluation** stage. Specifically, we will:  \n",
    "\n",
    "- Inspect the final **feature matrix (`X`)** and **target (`y`)**.  \n",
    "- Confirm the **dimensions** of the train/test splits after preprocessing.  \n",
    "- Preview the **transformed features** to check that both numerical (scaled) and categorical (one-hot encoded) variables are included.  \n",
    "- Verify that **stratified sampling** preserved the distribution of CEFR levels in both training and testing sets.\n",
    "\n",
    "This step ensures that the dataset is properly structured and balanced, which is essential for reliable model training and evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49ea408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (803, 13)\n",
      "X_test shape: (201, 13)\n",
      "y_train shape: (803,)\n",
      "y_test shape: (201,)\n",
      "\n",
      "Sample of transformed features (X_train):\n",
      "    strongest_skill weakest_skill second_weakest_skill  skill_std  \\\n",
      "575         reading       writing            listening   1.623618   \n",
      "576         writing      speaking            listening   0.903656   \n",
      "914        speaking       writing              reading  -0.343831   \n",
      "665       listening       writing              reading   0.519644   \n",
      "632         reading     listening             speaking  -0.115134   \n",
      "\n",
      "     strength_weakness_gap  productive_receptive_ratio    learning_profile  \\\n",
      "575               1.692987                   -1.607845  Uneven Development   \n",
      "576               1.162393                   -0.041896  Uneven Development   \n",
      "914              -0.429390                    0.027129            Balanced   \n",
      "665               0.631798                   -0.624777            Balanced   \n",
      "632              -0.164093                   -0.504772            Balanced   \n",
      "\n",
      "     speaking_minus_avg  reading_minus_avg  listening_minus_avg  \\\n",
      "575            0.266317           2.428122            -1.089884   \n",
      "576           -1.794698           0.397270            -0.421848   \n",
      "914            1.022023          -0.618156             0.446600   \n",
      "665            0.609820           0.058795             1.114636   \n",
      "632           -0.420688           1.480391            -0.689062   \n",
      "\n",
      "     writing_minus_avg  speaking_to_reading  writing_to_listening  \n",
      "575          -1.597783             0.800000              0.925926  \n",
      "576           1.811872             0.764706              1.258064  \n",
      "914          -0.847659             1.081081              0.935897  \n",
      "665          -1.802363             1.025316              0.867470  \n",
      "632          -0.370308             0.902778              1.015625  \n",
      "\n",
      "Target distribution (train): {np.int64(0): np.int64(166), np.int64(1): np.int64(175), np.int64(2): np.int64(166), np.int64(3): np.int64(154), np.int64(4): np.int64(75), np.int64(5): np.int64(67)}\n",
      "Target distribution (test): {np.int64(0): np.int64(42), np.int64(1): np.int64(44), np.int64(2): np.int64(42), np.int64(3): np.int64(38), np.int64(4): np.int64(19), np.int64(5): np.int64(16)}\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of train/test splits\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Preview feature matrix (first 5 rows)\n",
    "print(\"\\nSample of transformed features (X_train):\")\n",
    "print(X_train[:5])\n",
    "\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "train_dist = dict(zip(unique, counts))\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\nTarget distribution (train):\", train_dist)\n",
    "print(\"Target distribution (test):\", test_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02422bc7",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data  \n",
    "\n",
    "In this step, we will save the **final preprocessed dataset** so that it can be reused in the **Modelling & Evaluation notebook** without repeating all preprocessing steps.  \n",
    "\n",
    "We will:  \n",
    "- Save the **feature matrix (`X`)** and **target (`y`)** into CSV files.  \n",
    "- Store them inside the `data/processed/` folder for good project structure.  \n",
    "- Confirm that the files were saved successfully by reloading and inspecting their shapes.  \n",
    "\n",
    "This ensures reproducibility and keeps a clear separation between **raw**, **intermediate**, and **processed** data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e2ecbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed data saved successfully!\n",
      "Features shape: (1004, 13)\n",
      "Target shape: (1004,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create processed data folder if it doesn't exist\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Save processed features and target\n",
    "X.to_csv(\"data/processed/features.csv\", index=False)\n",
    "y.to_csv(\"data/processed/target.csv\", index=False)\n",
    "\n",
    "print(\"✅ Processed data saved successfully!\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216adc5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bb392",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2eb40f",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps  \n",
    "\n",
    "In this notebook, we:  \n",
    "\n",
    "- Imported and verified the **cleaned dataset**.  \n",
    "- Encoded the **target variable (CEFR levels)** into numerical labels.  \n",
    "- Engineered **8 new features** to capture learner skill profiles (e.g., strongest/weakest skill, learning profile, avg score, etc.).  \n",
    "- Scaled numeric features to ensure consistent ranges.  \n",
    "- Encoded categorical engineered features using **One-Hot Encoding (OHE)**.  \n",
    "- Performed a **stratified train/test split** to preserve CEFR class balance.  \n",
    "- Saved the **processed dataset** for future use.  \n",
    "\n",
    "✅ At this point, we have a **ready-to-use dataset** with features (`X`) and target (`y`) prepared for Machine Learning.  \n",
    "\n",
    "### 🚀 Next Steps (Modelling & Evaluation Notebook)\n",
    "In the next notebook, we will:  \n",
    "- Load the processed features and target.  \n",
    "- Build and train ML models (classification).  \n",
    "- Evaluate model performance (classification report, confusion matrix, etc.).  \n",
    "- Interpret results in terms of the **business requirements**.  \n",
    "- Prepare the foundation for generating **personalised learning recommendations**.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
