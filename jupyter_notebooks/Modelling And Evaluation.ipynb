{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebf79c1",
   "metadata": {},
   "source": [
    "# 05. Modelling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac5cb3",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to train, evaluate, and interpret machine learning models that predict **CEFR levels** based on learners’ language proficiency scores and engineered features. Specifically, we aim to:\n",
    "\n",
    "- Train classification models using the processed dataset (numeric + categorical engineered features).  \n",
    "- Compare model performance across multiple algorithms (e.g., Logistic Regression, Random Forest, Gradient Boosting).  \n",
    "- Evaluate models using accuracy, precision, recall, F1-score, and confusion matrices to assess classification reliability.  \n",
    "- Select the best-performing model for deployment and future integration into a personalized recommendation system.  \n",
    "- Save the trained model and evaluation results for reproducibility and downstream use.  \n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- **Processed dataset**: `data/processed/features.csv`  \n",
    "  - Includes original skill scores, engineered features (e.g., strongest/weakest skill, profile type), and encoded CEFR target variable.  \n",
    "- **Feature matrix (X)**: Scaled numeric features + encoded categorical engineered features.  \n",
    "- **Target vector (y)**: Encoded CEFR levels (A1–C2).  \n",
    "\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Trained machine learning models (baseline + advanced).  \n",
    "- Evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix).  \n",
    "- Visualizations of model performance.  \n",
    "- Final selected model, serialized (e.g., `model.joblib`) for reuse.  \n",
    "- Documentation of why the chosen model best supports the **business goal** of automatic learner placement.  \n",
    "\n",
    "\n",
    "## Additional Information\n",
    "\n",
    "This stage directly addresses the **business requirement**: predicting learners’ CEFR levels to enable **automatic placement** and **personalized learning recommendations**.  \n",
    "While the model outputs only the predicted CEFR level, the engineered features (skill strengths, weaknesses, balance profiles) provide the contextual insights needed for tailored feedback.  \n",
    "By rigorously evaluating different models and selecting the best one, we ensure predictions are not only accurate but also interpretable, reliable, and suitable for real-world integration into an adaptive learning platform.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801c026",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb9b54",
   "metadata": {},
   "source": [
    "# Project Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7500c31",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "\n",
    "We need to change the working directory from its current folder to the folder the code of this project is currently located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849c2560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\husse\\\\OneDrive\\\\Projects\\\\lang-level-pred\\\\jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8bbf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\husse\\OneDrive\\Projects\\lang-level-pred\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# swtich to project root directory\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0feac2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd7968",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "This code block imports fundamental Python libraries for data analysis and visualization and checks their versions\n",
    "\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computations\n",
    "- matplotlib: For creating visualizations and plots\n",
    "- seaborn: creating attractive and informative statistical graphics from datasets\n",
    "\n",
    "The version checks help ensure:\n",
    "- Code compatibility across different environments\n",
    "- Reproducibility of analysis\n",
    "- Easy debugging of version-specific issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1654478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.3.1\n",
      "NumPy version: 2.3.1\n",
      "matplotlib version: 3.10.5\n",
      "seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Import data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97126b98",
   "metadata": {},
   "source": [
    "### List Files and Folders\n",
    "- This code shows what files and folders are in our data/clean folder and what folder we are currently in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b6d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Files/folders available in data\\processed:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['features.csv', 'target.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = Path(\"data/processed\")\n",
    "print(f\"[INFO] Files/folders available in {dataset_dir}:\")\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f631987",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "In this step, we will load the processed dataset that was prepared in the Feature Engineering Notebook.  \n",
    "The data has been saved in two separate files:\n",
    "\n",
    "- `features.csv` → contains the engineered and scaled features.  \n",
    "- `target.csv` → contains the encoded CEFR levels.  \n",
    "\n",
    "We will:\n",
    "- Load both files.  \n",
    "- Inspect their structure (rows, columns, datatypes).  \n",
    "- Confirm they align correctly (same number of rows).  \n",
    "- Prepare them as `X` (features) and `y` (target) for model training.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12fcd570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1004, 12)\n",
      "Target shape: (1004,)\n",
      "\n",
      "Feature columns:\n",
      " ['speaking_score', 'reading_score', 'listening_score', 'writing_score', 'strongest_skill', 'weakest_skill', 'second_weakest_skill', 'skill_std', 'strength_weakness_gap', 'productive_receptive_ratio', 'avg_score', 'learning_profile']\n",
      "\n",
      "Target preview:\n",
      " 0    0\n",
      "1    4\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: cefr_encoded, dtype: int64\n",
      "✅ Features and target aligned correctly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed features and target\n",
    "X = pd.read_csv(\"data/processed/features.csv\")\n",
    "y = pd.read_csv(\"data/processed/target.csv\").squeeze()  # convert to Series\n",
    "\n",
    "# Inspect shapes\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nFeature columns:\\n\", X.columns.tolist())\n",
    "print(\"\\nTarget preview:\\n\", y.head())\n",
    "\n",
    "# Validate alignment\n",
    "assert X.shape[0] == y.shape[0], \"❌ Row mismatch between features and target!\"\n",
    "print(\"✅ Features and target aligned correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf2016",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf048b0",
   "metadata": {},
   "source": [
    "## 1. Define Features and Target\n",
    "\n",
    "Now that we have successfully loaded the processed dataset, we need to separate it into:\n",
    "\n",
    "- **X (features):** all engineered and scaled variables used by the model to make predictions.  \n",
    "- **y (target):** the encoded CEFR levels that the model will learn to predict.  \n",
    "\n",
    "We will confirm that both `X` and `y` are correctly structured and aligned before proceeding to train/test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b61bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix (X):\n",
      "   speaking_score  reading_score  listening_score  writing_score  \\\n",
      "0              24             38               30             34   \n",
      "1              93             91               90             89   \n",
      "2              62             64               64             55   \n",
      "3              63             59               54             54   \n",
      "4              79             74               85             79   \n",
      "\n",
      "  strongest_skill weakest_skill second_weakest_skill  skill_std  \\\n",
      "0         reading      speaking            listening   5.972158   \n",
      "1        speaking       writing            listening   1.707825   \n",
      "2         reading       writing             speaking   4.272002   \n",
      "3        speaking     listening            listening   4.358899   \n",
      "4       listening       reading             speaking   4.500000   \n",
      "\n",
      "   strength_weakness_gap  productive_receptive_ratio  avg_score  \\\n",
      "0                     14                    0.852941      31.50   \n",
      "1                      4                    1.005525      90.75   \n",
      "2                      9                    0.914062      61.25   \n",
      "3                      9                    1.035398      57.50   \n",
      "4                     11                    0.993711      79.25   \n",
      "\n",
      "     learning_profile  \n",
      "0  Uneven Development  \n",
      "1            Balanced  \n",
      "2            Balanced  \n",
      "3            Balanced  \n",
      "4            Balanced  \n",
      "\n",
      "Target vector (y):\n",
      "0    0\n",
      "1    4\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: cefr_encoded, dtype: int64\n",
      "\n",
      "Shapes:\n",
      "X: (1004, 12)\n",
      "y: (1004,)\n"
     ]
    }
   ],
   "source": [
    "# Confirm feature matrix (X) and target vector (y)\n",
    "\n",
    "print(\"Feature matrix (X):\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nTarget vector (y):\")\n",
    "print(y.head())\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5bd628",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split with One-Hot Encoding\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "- Split the dataset into **training** and **testing** sets using stratified sampling to preserve CEFR distribution.  \n",
    "- Identify categorical engineered features:  \n",
    "  - `strongest_skill`  \n",
    "  - `weakest_skill`  \n",
    "  - `second_weakest_skill`  \n",
    "  - `learning_profile`  \n",
    "- Apply **One-Hot Encoding (OHE)** to convert these categorical variables into numeric dummy variables.  \n",
    "- Keep the numeric features as they are.  \n",
    "- Verify the final transformed feature sets and check their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33359dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['strongest_skill', 'weakest_skill', 'second_weakest_skill', 'learning_profile']\n",
      "Numeric features: ['speaking_score', 'reading_score', 'listening_score', 'writing_score', 'skill_std', 'strength_weakness_gap', 'productive_receptive_ratio', 'avg_score']\n",
      "Training set size: (803, 12)\n",
      "Test set size: (201, 12)\n",
      "Processed training set shape: (803, 18)\n",
      "Processed test set shape: (201, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify categorical and numeric features\n",
    "categorical_features = [\"strongest_skill\", \"weakest_skill\", \"second_weakest_skill\", \"learning_profile\"]\n",
    "numeric_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "\n",
    "# Define preprocessing: One-Hot Encode categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "        (\"num\", \"passthrough\", numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the dataset (stratify ensures CEFR balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "\n",
    "# Fit and transform training set, transform test set\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Processed training set shape:\", X_train_processed.shape)\n",
    "print(\"Processed test set shape:\", X_test_processed.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
